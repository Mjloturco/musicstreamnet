\documentclass[11pt]{article}
\usepackage[paper  = letterpaper,
                     left   = 1.2in,
                     right  = 1.2in,
                     top    = 1.0in,
                     bottom = 1.0in,
                     ]{geometry}

% \usepackage[bitstream-charter]{mathdesign}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}



\title{CS 137 Project: Music Stream Count Prediction}
\author{Matthew Cook, Michael Loturco, and Abigail Stone}
\date{}

\begin{document}
\maketitle

\section{Introduction}

Online music streaming services produce a weath of data about song popularity and trends in music over time. Access to this data gives us the opportunity to try to predict popularity and growth of songs. Predicting the stream count trends of a song can be advantageous for artists, producers, and streaming services. Artists may find this information useful for understanding which of their songs are gaining the most popularity, and streaming services may be able to better allocate resources to serve that song to end-users.

Existing work \cite{araujo_predicting_2019, martin-gutierrez_multimodal_2020} has used metadata and audio features to assign a generalized performance prediction, such as ``low", ``medium" and ``high" popularity or ``hit song" and ``not hit song". The Spotify API \cite{noauthor_spotify_nodate} provides audio features (e.g. danciness and energy), lyrical features (e.g. valence and mood), and metadata for the track, including artist and album information. We are interested in forecasting the stream counts into future timesteps. Most of the existing literature focuses on computing a popularity score using metadata information, and we are interested in predicting stream count values over time. 

Since the popularity of a track changes over time, we hypothesize that estimating ``popularity" without considering the time dimension will produce poorer model performance than predicting popularity while considering time. 

First, we use basic LSTM and GRU models that use time series to predict stream counts in future time steps. Then, we propose a joint architecture that considers both the encoded metadata from each track and the time series data of stream counts of those songs over time. We hypothesize that by considering stream counts over time, predictions for popularity will be more accurate. 

We will evaluate our models using the standard metrics for regression problems: root mean squared error and mean absolute error. We will compare the results of our multiple-input model to the results of our own single-input LSTM and GRU models that mimic existing stream count prediction models in the literature.

% This section should movitate your project and introduce your project at a high level. As we have mentioned in the class, your project should contains some exploration. The exploration can be one or more of these aspects:  
% 
% \begin{itemize}
% \item applying deep models to a new problem;   
% \item devising a new machine learning for a new or existing problem;    
% \item implementing a model that does have official code; and 
% \item examining a particular learning technique.
% \end{itemize}
%
% In the motivation part, you want to show  
% \begin{itemize}
% \item why the problem you are solving is important; 
% \item which part of the project is the exploration beyond the literature;
% \end{itemize}
% \textbf{Your project should be more than repeating what has been done in the liaterature. }
% 
% Then you need to introduce your project. We may want to answer these questions: 
% \begin{itemize}
% \item what is the structure of the learning problem (the input, the predicting target, and the data);   
% \item what models to you plan to use;  
% \item how to measure the success of your project; and   
% \item are there any your hypotheses you need, and how hour hypotheses are verified and disputed? 
% \end{itemize}
% Please do NOT itemize answers to these questions because you need a coherent introduction of your project, not merely answers to these questions. 


\section{Related Work}

Martin-Gutierrez et. al \cite{martin-gutierrez_multimodal_2020} introduce the SpotGenTrack Popularity Dataset (SPD) and a deep learning architecture called HitMusicNet. SpotGenTrack combines metadata and audio data from Spotify and corresponding lyrical information from Genius and then extracts features from those sources. The extracted features are then compressed and encoded through a network MusicAENet. Finally a CNN (MusicPopNet) consumes the encoded and compressed feature vector to produce a classification of popularity. Similar previous models are noted to be binary classification, ``hit" vs ``not hit" whereas HitMusicNet is a ternary classifier separating ``low", ``medium" and ``high" classes.

% 
% \begin{figure}
%     \centering 
%     \includegraphics[width=5in]{figs/architecture.png}
%     \caption{Model architecture presented by Martin-Gutierrez et al \cite{martin-gutierrez_multimodal_2020}. We intend to replace the fully-connected CNN with an RNN.}
%     \label{architecture}
% \end{figure}


Li et al \cite{li_lstm-rpa_2021} present LSTM Rolling Prediction Algorithm (LSTM-RPA), a popularity prediction algorithm based on a Recurrent Neural Network. LSTM-RPA breaks the long-term prediction into several short-term trend predictions. This work is an important example of using an RNN architecture to predict song popularity. Wang et al \cite{wang_music_2019} also present an LSTM model for forecasting popularity.
    
Araujo et al \cite{araujo_predicting_2019} present a popularity prediction method using Support Vector Machines; this method predicts whether a song will appear on the Spotify Top 50 Global charts. This work has similar goals to our project and will serve as an important reference for evaluating our modelâ€™s accuracy.

In their 2018 paper, Choi et al \cite{choi_comparison_2018} compare audio pre-processing methods for deep neural networks. Their primary result demonstrates that many common preprocessing techniques are redundant; this conclusion will help us inform our pre-processing decisions for our project.

% 
% You need to study the current literature of your planned work. You don't have to exhaust the literature, but you need to search related work with a few proper key words.
% If there are related work, you need to cite them. For example, if you want to cite the ``Deep Learning'' book \cite{goodfellow2016deep}, then you should use the ``\textbackslash cite'' command.
% This section helps you to establish the current status of known knowledge around the project. Particularly, you may consider to cite the research work solving the same problem.  

% \section{Background} ???> Maybe??
% 
% This section is optional. If your project is related to some standard formulations, and you need to introduce details to make your later models clear, you should consider to put these \textbf{standard} formulations here.  For example, if you are going to do some modification of GRU later, but you need to have an introduction of GRU first, then it belongs to this section. A negaive example is, your project will use a GRU layer in a standard way, then a detailed introduction of GRU is not needed. 
% 
% 
% \noindent \textbf{Proposal submission:} When you submit your proposal, you should include the ``Introduction'' and ``Related Work'' sections. If you have a ``Background'' section, you should include it as well. The rubrics for grading your proposal include: 
% \begin{itemize}
% \item 2 points if you have a clear motivation
% \item 3 points if you have a clear summary of your project 
% \item 3 points if you have done at least a rough literature study 
% \item 2 points for the readability of your project (you can use Grammarly to correct your grammar issues.) 
% \end{itemize}
% 
\section{Methods}

\subsection{Dataset}
Our primary dataset is a modified version of the daily top 200 songs by stream count in the Philippines, from January 1st 2017 to May 20th 2021 \cite{peralta_spotify_nodate}. We reshaped this dataset to be of the form $\mathbb{R}^{d \times s}$, where $d$ is the number of dates in the dataset and $s$ is the number of songs.

One issue with this dataset is that it only represents the top 200 songs for a given day; as a result, songs occasionally drop off the charts for a few days and then return, leaving gaps in our time series. To solve this, we interpolated values for gaps of size exactly one (representing one day of off-chart stream counts) and filtered out songs containing zero values greater than a strict threshold. Due to the sparsity of our data, we experimented with different thresholds for including a song resulting, in a variety of different values for $s$.

% The format of the data at this point is tensor of the shape [Timesteps, Stream Count By Song]. The data was still incredibly sparse and in order to further avoid this problem and to form a training and testing set we scanned out data sets for songs which achieved a certain threshold entries over the course of the time.

To formulate our training data, we took sub-sequences of the overall date sequence for each song, with subsequences each of size \texttt{lookback} in our training dataset and the next timestep (of length one) set as the target value.

 After trimming our data set in such a manner the shape is 
\[X = [\text{Batch Size, Lookback, $s$}]\]
\[Y = [\text{Batch Size, 1, $s$}]\]
where $X$ is our training data and $Y$ is our target data.

In order to test our hypothesis, one of our architectures has multiple inputs. We augmented the existing dataset with additional features using the Spotify API. For each track in the existing dataset, we added album and artist information, as well as audio features from the Spotify API such as dancability, acousticness, energy, and loudness, among others. These audio features are all real values between 0 and 1. For each of the songs, we extracted a total of 13 metadata features and produced a tensor of shape [$s$, 13].

\subsection{Model Architecture}

The learning problem is a straightforward regression problem. Given the daily stream counts of songs over a period of time, we are looking to accurately predict the stream count of the next day. Given the time-oriented nature of the problem, our models consist of recurrent layers. We built basic models using both LSTMs and GRUs to produce baseline accuracy metrics to be used to evaluate our multiple-input models. 

We propose an architecture for a multi-input model that combines stream-count data with the extracted metadata. We first run the stream count information through a recurrent neural network. Then we concatenate the output from this model with the $m$ metadata features to create a feature vector of length $m + 1$. Then we feed this feature vector through a feed-forward neural network to output an improved stream count prediction.

\begin{figure}
    \centering{}
    \includegraphics[width=4in]{figs/model_arch_sketch.jpg}
    \caption{Multiple-Input model diagram}
\end{figure}

\subsection{Model Evaluation}
We used Mean Squared Error, Root Mean Squared Error, and Mean Absolute Error as our evaluation metrics. 

Additionally, we visualize the predictions of our model by plotting the predictions alongside the original stream counts.

We set up two baseline models to evaluate the performance of our multi-input model: a basic LSTM implenetation and a basic GRU model to predict stream counts based solely on time series.


% 
% This section should be a detailed introduction of your project. This section should only contain your own work. For example, if you have modified a GRU architecture, this section should only have the modification of GRU. The original GRU structure should be put in the ``Background'' section.  
% 
% In this section, you should have a rigorous introduction of your problem. This list of questions help you to do the formulation,
% \begin{itemize}
% \item What data do you have? How are they denoted ($X$ represents features)? Is every notation well defined (e.g. $X \in \mathcal{R}^d$)? 
% \item What type learning problem it is? What's the desired output from the model? 
% \item What's the input to the model? Is it the same as (part of) the data? Any processing before you can feed the data to the model?
% \item What the model? If the model is not a standard one, do you have a function form of the model?
% \item What's the loss function of the model? If it is not a standard problem, you need to use an equation to show that exactly. 
% \item Can you check all symbols to make sure that every one is well defined?
% \end{itemize}
% 
% 
\section{Experiments}

\subsection{GRU and LSTM}

We performed a parameter search over two key dimensions. One parameter we varied was look back, the length of the time sequence provided which. Figure \ref{lookbackfig} (a) shows how MSE varies as look back changes in a GRU model. Figure \ref{lookbackfig} (b) shows the same variables searched over an LSTM model.

We note that there is not a clear performance trend based on lookback, but rather it appears to be periodic. We theorize this could potentially be due to stream count behavior appearing to be periodic as well, following somewhat consistent bumps of length 7 as the counts follow a pattern from week to week. Additionally, in the GRU model, as the song count increases, the performance becomes more stable and RMSE varies less over lookback. 

We also note that for the LSTM model, that predictions of models trained on sequences on only a single song appear to outperform models trained on sequences from multiple songs. This was counter to an initial hypothesis of ours, where we expected a model to perform better by being able to learn from multiple songs. This potentially indicates that songs do not share enough sequence similarity to be beneficial to our model, and instead the variety of sequences introduced noise that caused the model to perform worse. 


\begin{figure}[H]
    \centering
    \includegraphics[width=3in]{figs/gru_lookback}
    \includegraphics[width=3in]{figs/lstm_lookback}
    \caption{Lookback experiment results for (a) GRU (left) and (b) LSTM (right)}
    \label{lookbackfig}
\end{figure}


\begin{figure}[h]
    \centering
    \begin{tabular}{| c  c  c  c|}
        \hline
         & LSTM  & GRU  & Multiple-Input\\ 
         \hline 
         RMSE & 3,460 & 3,959  & 7,742 \\ 
         MAE & 2,369 & 2,777 &  7,159 \\  
         \hline
    \end{tabular}
    \caption{Root Mean Squared Error and Mean Absolute Error for our models}
    \label{results}
\end{figure}

\begin{figure}[h]
    \centering 
    \includegraphics[width=3in]{figs/lstm_loss.png}
    \includegraphics[width=3in]{figs/gru_loss.png}
    \caption{Training and Testing loss (MSE) for LSTM (left) and GRU (right)}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=3in ]{figs/lstm_predicted}
    \includegraphics[width=3in]{figs/gru_predicted}
    \caption{Predicted stream counts and target data for LSTM (left) and GRU (right)}
\end{figure}

\subsection{Multiple-Input Models}
% TODO: results for joint model 

% TODO: put in shapes here 
Our multiple-input model is a Keras functional model that takes the stream count data as input to an LSTM, concatenates the result of the recurrent network to a vector of 13 metadata features, and feeds this result through a feed-forward network. 

The results of this model (see Figure \ref{results}) demonstrate that the addition of metadata features did not improve the stream-count prediction. 

\begin{figure}[h]
    \centering 
    \includegraphics[width=3in]{figs/loss}
    \caption{Training and Testing loss for mutliple-input model }
\end{figure}


% Our first major attempt to improve performance was to train the multiple input model, which was previously described. This did not yield satisfactory results and was not properly training. % TODO: revisit this as a group 
% 
%  There are two intuition as to why the model did not work. The first being that since the recurrent layers are deeper in the model then we have no control over how they train. Therefore they may not be training to predict stream count and rather some other value. The other major issue is that since the RNN is only producing one value which is being concatenated onto the metadata for each song, than the model is going to naturally give more weight to the 13 metadata variables. 
% 
% TODO: sample graphs 
% TODO: output metrics 
% TODO: comparison between sequential, functional, meta, no meta
% TODO: other model tweaks 


% TODO: plots here 

% 
% In this section, you should report your experiments, which that are either related to your hypotheses or the evaluation measures of your models. Do NOT manipulate experiment results, and its OK that the experiment results do not support your hypotheses or expectations. For example, if your project is a modification of the GRU and argues that the motivation can improve its performance. However, the experiment results do not show so. In this section, you should report your observations in experiments. Then you should analyze the results in either of the following ways:
% \begin{itemize}
% \item the original argument is still correct, but experiments are not perfect. There is still a chance that the modification should work.  
% \item the original argument is invalid by observing experiment results. Some part of the argument is wrong. If there is a fix, you can also talk about further modifications.  
% \end{itemize}
% 
\section{Conclusion}

Through these experiments, we found that our baseline models were able to predict stream counts for the next time steps. Given the parameters that we identified through a grid search, we found that our LSTM model out-performed our GRU model. 

When compared to our baseline models, our multi-input model did not have better predictive performance. This contradicts our initial hypothesis that the inclusion of metadata with the time-series stream count data would improve the predictive performance. Instead, we found that our multi-input model predicted a constant value over all time steps. We hypothesize that this is due to a feature imbalance between our time-series data and metadata when combined into a feature vector. Since the metadata stays constant across all timesteps, the model prediction was less able to distinguish variations over time.

Future work could investigate rectifying this imbalance between time and metadata features. Additionally, other models could consider training time-series and metadata networks separately before combining features to feed into the final regression model. Additional experiments may also benefit from training on less sparse data. Despite careful data selection for our models, we were limited by the format of a Top 200 charts dataset, which inevitably had missing data points for many songs.

Additionally, training on songs that are of lower popularity than the top 200 songs might be able to better predict stream count across all levels of popularity; this is likely to be more beneficial for artists, producers, and streaming services.

% 
% While we were unsuccessful in our attempts to use the metadata of the song to improve performance that may mostly be due to implementation issues. As such it could certainly benefit from further attempts. One potential modification to the multi-input model is to train the recurrent layers as a separate model for a preliminary input. Then proceed to train a model which uses that output alongside the metadata to refine the predicted value. This avoids the issue of not knowing how the recurrent layer is training in the larger model, but still may present the problem that more weight is being given to the metadata than the predicted value. As well a larger and less sparse data set could be used a most likely would improve performance.
% While adding more data would improve the performance it could also lead to complications due to differing music tastes across different regions. By focusing on the Philippines data set  

% TODO: which models were successful and how successful 
% TODO: reiterate the WHY 
% TODO: future work discussion 

% Draw conclusions from this project. Summarize the new findings you want your reader to learn.  
% 
\section{Share of work}

Overall, we believe that work was divided equitably.
% 
% If the team has more than person, then you should report who has done which part of work. Particularly, you should mention these work items. If multiple people participated in one item, then you can list the percentage of contribution of each person. 
% 
\begin{itemize}
\item writing of the proposal  (all)
\item dataset creation and augmentation (Abigail 80\%, Michael 20\%)
\item coding (Abigail 40\%, Michael 40\%, Matthew 20\%)
% \item running experiments and collecting data 
\item discussions (very evenly split) 
\item writing of the final report (Matthew 40\%, Abigail 40\%, Michael 20\%)
\end{itemize}

\bibliographystyle{plain}
\bibliography{sample}

\end{document}
